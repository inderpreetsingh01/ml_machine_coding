{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQv8J3fA2Z0tubCX+Av8v/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inderpreetsingh01/ml_machine_coding/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsYvKYAjMNmH"
      },
      "outputs": [],
      "source": [
        "# Binary classification\n",
        "# Batch Gradient Descent\n",
        "# Sigmoid activation\n",
        "# Cross-Entropy Loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "AZcpyDCiNEb3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature\tIncluded\n",
        "# Sigmoid Activation\t✅\n",
        "# Gradient Descent\t✅\n",
        "# Binary Classification\t✅\n",
        "# Log Loss & Accuracy Metrics\t✅\n",
        "# Predict Probabilities\t✅\n",
        "# Predict Class Labels\t✅\n",
        "\n",
        "class LogisticRegression:\n",
        "    \"\"\"\n",
        "    Basic Logistic Regression using Batch Gradient Descent.\n",
        "    No Regularization, No Scaling, No Early Stopping.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, n_iters=1000):\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Sigmoid activation function.\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train Logistic Regression model.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_pred = self._sigmoid(linear_model)\n",
        "\n",
        "            # Gradients\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
        "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            # Update weights\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict probabilities for class 1.\n",
        "        \"\"\"\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        return self._sigmoid(linear_model)\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        \"\"\"\n",
        "        Predict binary class labels.\n",
        "        \"\"\"\n",
        "        y_pred_proba = self.predict_proba(X)\n",
        "        return (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "    def evaluate(self, X, y_true):\n",
        "        \"\"\"\n",
        "        Evaluate model using accuracy and log loss.\n",
        "        \"\"\"\n",
        "        y_pred_proba = self.predict_proba(X)\n",
        "        y_pred_labels = self.predict(X)\n",
        "        accuracy = np.mean(y_true == y_pred_labels)\n",
        "        log_loss = -np.mean(\n",
        "            y_true * np.log(y_pred_proba + 1e-15) + (1 - y_true) * np.log(1 - y_pred_proba + 1e-15)\n",
        "        )\n",
        "        return {\"accuracy\": accuracy, \"log_loss\": log_loss}\n",
        "\n",
        "\n",
        "# ==== Synthetic Test Case ====\n",
        "np.random.seed(42)\n",
        "n_samples = 100\n",
        "X = np.random.randn(n_samples, 2)\n",
        "true_weights = np.array([2, -1])\n",
        "bias = -0.5\n",
        "linear_combination = np.dot(X, true_weights) + bias\n",
        "y = (linear_combination > 0).astype(int)  # Generate binary labels\n",
        "\n",
        "print(\"==== Training Basic Logistic Regression ====\")\n",
        "model = LogisticRegression(lr=0.1, n_iters=1000)\n",
        "model.fit(X, y)\n",
        "metrics = model.evaluate(X, y)\n",
        "print(\"Weights:\", model.weights)\n",
        "print(\"Bias:\", model.bias)\n",
        "print(\"Metrics:\", metrics)"
      ],
      "metadata": {
        "id": "8LiRL7VSMrwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kda2jxIaMw5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V77GAe8EMyoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# L1 and L2 regularization\n",
        "class LogisticRegression:\n",
        "    \"\"\"\n",
        "    Logistic Regression with L1 (Lasso) and L2 (Ridge) Regularization.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, n_iters=1000, l1_lambda=0.0, l2_lambda=0.0):\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.l1_lambda = l1_lambda\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_pred = self._sigmoid(linear_model)\n",
        "\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
        "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            # Add Regularization\n",
        "            dw += self.l2_lambda * self.weights  # L2\n",
        "            dw += self.l1_lambda * np.sign(self.weights)  # L1\n",
        "\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        return self._sigmoid(linear_model)\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        return (self.predict_proba(X) >= threshold).astype(int)\n",
        "\n",
        "    def evaluate(self, X, y_true):\n",
        "        y_pred_proba = self.predict_proba(X)\n",
        "        y_pred_labels = self.predict(X)\n",
        "        accuracy = np.mean(y_true == y_pred_labels)\n",
        "        log_loss = -np.mean(\n",
        "            y_true * np.log(y_pred_proba + 1e-15) + (1 - y_true) * np.log(1 - y_pred_proba + 1e-15)\n",
        "        )\n",
        "        return {\"accuracy\": accuracy, \"log_loss\": log_loss}\n",
        "\n",
        "\n",
        "# ==== Synthetic Test Case ====\n",
        "np.random.seed(42)\n",
        "n_samples = 100\n",
        "X = np.random.randn(n_samples, 2)\n",
        "true_weights = np.array([2, -1])\n",
        "bias = -0.5\n",
        "linear_combination = np.dot(X, true_weights) + bias\n",
        "y = (linear_combination > 0).astype(int)\n",
        "\n",
        "print(\"==== Logistic Regression with L1 & L2 Regularization ====\")\n",
        "model = LogisticRegression(lr=0.1, n_iters=1000, l1_lambda=0.1, l2_lambda=0.1)\n",
        "model.fit(X, y)\n",
        "metrics = model.evaluate(X, y)\n",
        "print(\"Weights:\", model.weights)\n",
        "print(\"Bias:\", model.bias)\n",
        "print(\"Metrics:\", metrics)"
      ],
      "metadata": {
        "id": "_VZKvJleMylK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r2OtkGBHMyfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Why Feature Scaling?\n",
        "# Speeds up convergence of gradient descent.\n",
        "# Prevents domination by features with large magnitudes.\n",
        "# Especially critical for Logistic Regression."
      ],
      "metadata": {
        "id": "JVUYnY2xMycR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "    \"\"\"\n",
        "    Logistic Regression with:\n",
        "    - L1 & L2 Regularization\n",
        "    - Feature Scaling (Standardization)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, n_iters=1000, l1_lambda=0.0, l2_lambda=0.0):\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.l1_lambda = l1_lambda\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.scaler_mean = None\n",
        "        self.scaler_std = None\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def _scale_features(self, X, fit=True):\n",
        "        \"\"\"\n",
        "        Standardize features (Z-score normalization).\n",
        "        \"\"\"\n",
        "        if fit:\n",
        "            self.scaler_mean = np.mean(X, axis=0)\n",
        "            self.scaler_std = np.std(X, axis=0)\n",
        "            self.scaler_std[self.scaler_std == 0] = 1  # Avoid division by zero\n",
        "        return (X - self.scaler_mean) / self.scaler_std\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X_scaled = self._scale_features(X, fit=True)\n",
        "        n_samples, n_features = X_scaled.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            linear_model = np.dot(X_scaled, self.weights) + self.bias\n",
        "            y_pred = self._sigmoid(linear_model)\n",
        "\n",
        "            dw = (1 / n_samples) * np.dot(X_scaled.T, (y_pred - y))\n",
        "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            # Regularization\n",
        "            dw += self.l2_lambda * self.weights\n",
        "            dw += self.l1_lambda * np.sign(self.weights)\n",
        "\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X_scaled = self._scale_features(X, fit=False)\n",
        "        linear_model = np.dot(X_scaled, self.weights) + self.bias\n",
        "        return self._sigmoid(linear_model)\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        return (self.predict_proba(X) >= threshold).astype(int)\n",
        "\n",
        "    def evaluate(self, X, y_true):\n",
        "        y_pred_proba = self.predict_proba(X)\n",
        "        y_pred_labels = self.predict(X)\n",
        "        accuracy = np.mean(y_true == y_pred_labels)\n",
        "        log_loss = -np.mean(\n",
        "            y_true * np.log(y_pred_proba + 1e-15) + (1 - y_true) * np.log(1 - y_pred_proba + 1e-15)\n",
        "        )\n",
        "        return {\"accuracy\": accuracy, \"log_loss\": log_loss}\n",
        "\n",
        "\n",
        "# ==== Synthetic Test Case ====\n",
        "np.random.seed(42)\n",
        "n_samples = 100\n",
        "X = np.random.randn(n_samples, 2) * 10  # Scaled features to test scaling\n",
        "true_weights = np.array([2, -1])\n",
        "bias = -0.5\n",
        "linear_combination = np.dot(X, true_weights) + bias\n",
        "y = (linear_combination > 0).astype(int)\n",
        "\n",
        "print(\"==== Logistic Regression with L1/L2 Regularization and Feature Scaling ====\")\n",
        "model = LogisticRegression(lr=0.1, n_iters=1000, l1_lambda=0.1, l2_lambda=0.1)\n",
        "model.fit(X, y)\n",
        "metrics = model.evaluate(X, y)\n",
        "print(\"Weights:\", model.weights)\n",
        "print(\"Bias:\", model.bias)\n",
        "print(\"Metrics:\", metrics)"
      ],
      "metadata": {
        "id": "Kp_78uhGMyYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_logistic_regression():\n",
        "    np.random.seed(42)\n",
        "    # Synthetic binary classification: y = 1 if 2*x1 - x2 - 0.5 > 0 else 0\n",
        "    X = np.random.randn(100, 2) * 10  # Test scaling as well\n",
        "    true_weights = np.array([2, -1])\n",
        "    bias = -0.5\n",
        "    y = (np.dot(X, true_weights) + bias > 0).astype(int)\n",
        "\n",
        "    model = LogisticRegression(\n",
        "        lr=0.1,\n",
        "        n_iters=1000,\n",
        "        l1_lambda=0.1,\n",
        "        l2_lambda=0.1\n",
        "    )\n",
        "    model.fit(X, y)\n",
        "    metrics = model.evaluate(X, y)\n",
        "\n",
        "    # ✅ Test assertions\n",
        "    assert metrics['accuracy'] >= 0.95, f\"Expected high accuracy, got {metrics['accuracy']}\"\n",
        "    assert metrics['log_loss'] < 0.2, f\"Expected low log loss, got {metrics['log_loss']}\"\n",
        "    print(\"✅ Logistic Regression Test Passed:\", metrics)"
      ],
      "metadata": {
        "id": "CYZ22KKaMyWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RSj8TisU-SUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_-Dx9XTH-SS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function calculation and early stopping\n",
        "class LogisticRegression:\n",
        "    \"\"\"\n",
        "    Logistic Regression with:\n",
        "    - L1 & L2 Regularization\n",
        "    - Feature Scaling\n",
        "    - Binary Cross-Entropy Loss Calculation\n",
        "    - Early Stopping\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, n_iters=1000, l1_lambda=0.0, l2_lambda=0.0,\n",
        "                 early_stopping=False, tol=1e-4, patience=10):\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.l1_lambda = l1_lambda\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.early_stopping = early_stopping\n",
        "        self.tol = tol\n",
        "        self.patience = patience\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.scaler_mean = None\n",
        "        self.scaler_std = None\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def _scale_features(self, X, fit=True):\n",
        "        if fit:\n",
        "            self.scaler_mean = np.mean(X, axis=0)\n",
        "            self.scaler_std = np.std(X, axis=0)\n",
        "            self.scaler_std[self.scaler_std == 0] = 1\n",
        "        return (X - self.scaler_mean) / self.scaler_std\n",
        "\n",
        "    def _compute_loss(self, y_true, y_pred):\n",
        "        # Binary Cross Entropy Loss + Regularization\n",
        "        bce_loss = -np.mean(\n",
        "            y_true * np.log(y_pred + 1e-15) + (1 - y_true) * np.log(1 - y_pred + 1e-15)\n",
        "        )\n",
        "        l1_penalty = self.l1_lambda * np.sum(np.abs(self.weights))\n",
        "        l2_penalty = self.l2_lambda * 0.5 * np.sum(self.weights ** 2)\n",
        "        return bce_loss + l1_penalty + l2_penalty\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X_scaled = self._scale_features(X, fit=True)\n",
        "        n_samples, n_features = X_scaled.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        best_loss = np.inf\n",
        "        patience_counter = 0\n",
        "\n",
        "        for i in range(self.n_iters):\n",
        "            linear_model = np.dot(X_scaled, self.weights) + self.bias\n",
        "            y_pred = self._sigmoid(linear_model)\n",
        "\n",
        "            # Gradients\n",
        "            dw = (1 / n_samples) * np.dot(X_scaled.T, (y_pred - y))\n",
        "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            # Regularization Gradients\n",
        "            dw += self.l2_lambda * self.weights\n",
        "            dw += self.l1_lambda * np.sign(self.weights)\n",
        "\n",
        "            # Update\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "            # Calculate Loss for Early Stopping\n",
        "            if self.early_stopping:\n",
        "                loss = self._compute_loss(y, y_pred)\n",
        "                if loss + self.tol < best_loss:\n",
        "                    best_loss = loss\n",
        "                    patience_counter = 0\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if patience_counter >= self.patience:\n",
        "                        print(f\"Early stopping at iteration {i}, Loss: {loss:.4f}\")\n",
        "                        break\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X_scaled = self._scale_features(X, fit=False)\n",
        "        linear_model = np.dot(X_scaled, self.weights) + self.bias\n",
        "        return self._sigmoid(linear_model)\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        return (self.predict_proba(X) >= threshold).astype(int)\n",
        "\n",
        "    def evaluate(self, X, y_true):\n",
        "        y_pred_proba = self.predict_proba(X)\n",
        "        y_pred_labels = self.predict(X)\n",
        "        accuracy = np.mean(y_true == y_pred_labels)\n",
        "        log_loss = self._compute_loss(y_true, y_pred_proba)\n",
        "        return {\"accuracy\": accuracy, \"log_loss\": log_loss}"
      ],
      "metadata": {
        "id": "0IOFCa9c-SRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Id0nn7RW-SPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multiclass classification problem\n",
        "\n",
        "# ✅ Key Notes:\n",
        "# Uses one-hot encoding internally for gradient computation.\n",
        "# Loss printed every 100 iterations for monitoring.\n",
        "# Numerically stable softmax (avoids overflow).\n",
        "# Supports easy switching between binary & multi-class via softmax.\n",
        "\n",
        "class SoftmaxRegression:\n",
        "    \"\"\"\n",
        "    Multiclass Logistic Regression (Softmax Classifier)\n",
        "    - Supports L2 Regularization\n",
        "    - Feature Scaling (Standardization)\n",
        "    \"\"\"\n",
        "    def __init__(self, lr=0.1, n_iters=1000, l2_lambda=0.0):\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.scaler_mean = None\n",
        "        self.scaler_std = None\n",
        "\n",
        "    def _softmax(self, z):\n",
        "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Stability\n",
        "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "    def _scale_features(self, X, fit=True):\n",
        "        if fit:\n",
        "            self.scaler_mean = np.mean(X, axis=0)\n",
        "            self.scaler_std = np.std(X, axis=0)\n",
        "            self.scaler_std[self.scaler_std == 0] = 1\n",
        "        return (X - self.scaler_mean) / self.scaler_std\n",
        "\n",
        "    def _compute_loss(self, y_true, y_pred):\n",
        "        n_samples = y_true.shape[0]\n",
        "        # Cross-Entropy Loss with L2 Regularization\n",
        "        log_probs = -np.log(y_pred[range(n_samples), y_true] + 1e-15)\n",
        "        ce_loss = np.mean(log_probs)\n",
        "        l2_penalty = self.l2_lambda * 0.5 * np.sum(self.weights ** 2)\n",
        "        return ce_loss + l2_penalty\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X_scaled = self._scale_features(X, fit=True)\n",
        "        n_samples, n_features = X_scaled.shape\n",
        "        n_classes = np.max(y) + 1\n",
        "\n",
        "        self.weights = np.zeros((n_features, n_classes))\n",
        "        self.bias = np.zeros(n_classes)\n",
        "\n",
        "        for i in range(self.n_iters):\n",
        "            logits = np.dot(X_scaled, self.weights) + self.bias\n",
        "            y_pred = self._softmax(logits)\n",
        "\n",
        "            # One-hot encoding of y\n",
        "            y_one_hot = np.zeros_like(y_pred)\n",
        "            y_one_hot[np.arange(n_samples), y] = 1\n",
        "\n",
        "            # Gradients\n",
        "            dw = (1 / n_samples) * np.dot(X_scaled.T, (y_pred - y_one_hot))\n",
        "            db = (1 / n_samples) * np.sum(y_pred - y_one_hot, axis=0)\n",
        "\n",
        "            # L2 Regularization\n",
        "            dw += self.l2_lambda * self.weights\n",
        "\n",
        "            # Update weights & bias\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "            # Optional: Loss monitoring\n",
        "            if i % 100 == 0 or i == self.n_iters - 1:\n",
        "                loss = self._compute_loss(y, y_pred)\n",
        "                print(f\"Iteration {i}, Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X_scaled = self._scale_features(X, fit=False)\n",
        "        logits = np.dot(X_scaled, self.weights) + self.bias\n",
        "        return self._softmax(logits)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = self.predict_proba(X)\n",
        "        return np.argmax(y_pred, axis=1)\n",
        "\n",
        "    def evaluate(self, X, y_true):\n",
        "        y_pred = self.predict(X)\n",
        "        accuracy = np.mean(y_true == y_pred)\n",
        "        return {\"accuracy\": accuracy}"
      ],
      "metadata": {
        "id": "sjtV2YHV-SE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# Synthetic dataset (3 classes)\n",
        "n_samples = 300\n",
        "n_features = 2\n",
        "n_classes = 3\n",
        "X = np.random.randn(n_samples, n_features) * 2\n",
        "y = np.random.choice(n_classes, n_samples)\n",
        "\n",
        "model = SoftmaxRegression(lr=0.1, n_iters=1000, l2_lambda=0.01)\n",
        "model.fit(X, y)\n",
        "metrics = model.evaluate(X, y)\n",
        "print(\"Weights:\\n\", model.weights)\n",
        "print(\"Bias:\", model.bias)\n",
        "print(\"Metrics:\", metrics)"
      ],
      "metadata": {
        "id": "tDVce0Va-SCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r82l06Nz_Fty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multilabel classification\n",
        "import numpy as np\n",
        "\n",
        "class MultiLabelLogisticRegression:\n",
        "    \"\"\"\n",
        "    Multilabel Logistic Regression using Sigmoid per class\n",
        "    - Each label is treated as a binary classification task\n",
        "    - Supports L2 Regularization\n",
        "    - Feature Scaling included\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.1, n_iters=1000, l2_lambda=0.0):\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.scaler_mean = None\n",
        "        self.scaler_std = None\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def _scale_features(self, X, fit=True):\n",
        "        if fit:\n",
        "            self.scaler_mean = np.mean(X, axis=0)\n",
        "            self.scaler_std = np.std(X, axis=0)\n",
        "            self.scaler_std[self.scaler_std == 0] = 1\n",
        "        return (X - self.scaler_mean) / self.scaler_std\n",
        "\n",
        "    def _compute_loss(self, y_true, y_pred):\n",
        "        # Binary Cross Entropy loss for each class\n",
        "        bce_loss = -np.mean(y_true * np.log(y_pred + 1e-15) +\n",
        "                            (1 - y_true) * np.log(1 - y_pred + 1e-15))\n",
        "        l2_penalty = self.l2_lambda * 0.5 * np.sum(self.weights ** 2)\n",
        "        return bce_loss + l2_penalty\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"\n",
        "        X: shape (n_samples, n_features)\n",
        "        Y: shape (n_samples, n_classes), binary 0/1\n",
        "        \"\"\"\n",
        "        X_scaled = self._scale_features(X, fit=True)\n",
        "        n_samples, n_features = X_scaled.shape\n",
        "        n_classes = Y.shape[1]\n",
        "\n",
        "        self.weights = np.zeros((n_features, n_classes))\n",
        "        self.bias = np.zeros(n_classes)\n",
        "\n",
        "        for i in range(self.n_iters):\n",
        "            linear = np.dot(X_scaled, self.weights) + self.bias\n",
        "            y_pred = self._sigmoid(linear)\n",
        "\n",
        "            # Gradients\n",
        "            dw = (1 / n_samples) * np.dot(X_scaled.T, (y_pred - Y))\n",
        "            db = (1 / n_samples) * np.sum(y_pred - Y, axis=0)\n",
        "\n",
        "            # L2 regularization\n",
        "            dw += self.l2_lambda * self.weights\n",
        "\n",
        "            # Update\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "            if i % 100 == 0 or i == self.n_iters - 1:\n",
        "                loss = self._compute_loss(Y, y_pred)\n",
        "                print(f\"Iteration {i}, Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X_scaled = self._scale_features(X, fit=False)\n",
        "        logits = np.dot(X_scaled, self.weights) + self.bias\n",
        "        return self._sigmoid(logits)\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        proba = self.predict_proba(X)\n",
        "        return (proba >= threshold).astype(int)\n",
        "\n",
        "    def evaluate(self, X, Y_true, threshold=0.5):\n",
        "        Y_pred = self.predict(X, threshold)\n",
        "        accuracy = np.mean(Y_pred == Y_true)\n",
        "        return {\"accuracy\": accuracy}"
      ],
      "metadata": {
        "id": "a5m8pUpz_Fqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# 100 samples, 5 features, 3 labels\n",
        "X = np.random.randn(100, 5)\n",
        "Y = np.random.randint(0, 2, size=(100, 3))  # multilabel ground truth\n",
        "\n",
        "model = MultiLabelLogisticRegression(lr=0.1, n_iters=1000)\n",
        "model.fit(X, Y)\n",
        "\n",
        "proba = model.predict_proba(X)\n",
        "preds = model.predict(X)\n",
        "metrics = model.evaluate(X, Y)\n",
        "print(\"Metrics:\", metrics)"
      ],
      "metadata": {
        "id": "Ho80TNlb_Flo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XXgJfsWj_FkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1C5QbdZWRVvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ak_2w6H0RVr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UtXzoDJ7RVpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_auc_riemann(y_true, y_scores, method=\"left\"):\n",
        "    thresholds = np.linspace(0, 1, 100)\n",
        "    tpr_list, fpr_list = [], []\n",
        "\n",
        "    for thresh in thresholds:\n",
        "        y_pred = (y_scores >= thresh).astype(int)\n",
        "        TP = np.sum((y_true == 1) & (y_pred == 1))\n",
        "        TN = np.sum((y_true == 0) & (y_pred == 0))\n",
        "        FP = np.sum((y_true == 0) & (y_pred == 1))\n",
        "        FN = np.sum((y_true == 1) & (y_pred == 0))\n",
        "\n",
        "        tpr = TP / (TP + FN + 1e-15)\n",
        "        fpr = FP / (FP + TN + 1e-15)\n",
        "        tpr_list.append(tpr)\n",
        "        fpr_list.append(fpr)\n",
        "\n",
        "    tpr = np.array(tpr_list)\n",
        "    fpr = np.array(fpr_list)\n",
        "\n",
        "    # Sort by FPR\n",
        "    sorted_idx = np.argsort(fpr)\n",
        "    fpr = fpr[sorted_idx]\n",
        "    tpr = tpr[sorted_idx]\n",
        "\n",
        "    auc = 0\n",
        "    for i in range(len(fpr) - 1):\n",
        "        width = fpr[i+1] - fpr[i]\n",
        "        height = tpr[i] if method == \"left\" else tpr[i+1]\n",
        "        auc += width * height\n",
        "\n",
        "    return auc\n",
        "\n",
        "\n",
        "np.random.seed(0)\n",
        "y_true = np.array([0, 1, 1, 0, 1, 0, 1])\n",
        "y_scores = np.array([0.1, 0.8, 0.65, 0.2, 0.95, 0.4, 0.9])\n",
        "\n",
        "fpr, tpr, auc = compute_roc_auc(y_true, y_scores)\n",
        "print(f\"AUC Score: {auc:.4f}\")"
      ],
      "metadata": {
        "id": "N_1TcgfN_Fa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_auc_trapezoid(y_true, y_scores):\n",
        "    fpr, tpr, _ = compute_roc_auc(y_true, y_scores)\n",
        "    auc = np.trapz(tpr, fpr)\n",
        "    return auc"
      ],
      "metadata": {
        "id": "v_u-oA3HRTog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m2zLvDlARTlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_classification_metrics(y_true, y_pred, y_proba=None):\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
        "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
        "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
        "\n",
        "    accuracy = (TP + TN) / len(y_true)\n",
        "    precision = TP / (TP + FP + 1e-15)\n",
        "    recall = TP / (TP + FN + 1e-15)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-15)\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1\n",
        "    }\n",
        "\n",
        "    # ROC-AUC (only if probabilities provided)\n",
        "    if y_proba is not None:\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
        "        auc = np.trapz(tpr, fpr)\n",
        "        metrics[\"roc_auc\"] = auc\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# Example binary test\n",
        "np.random.seed(0)\n",
        "y_true = np.array([0, 1, 1, 0, 1, 0, 1])\n",
        "y_proba = np.array([0.1, 0.8, 0.65, 0.2, 0.95, 0.4, 0.9])\n",
        "y_pred = (y_proba >= 0.5).astype(int)\n",
        "\n",
        "metrics = binary_classification_metrics(y_true, y_pred, y_proba)\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "7c_da9abRTjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YcUI-e-NRTf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_accuracy(y_true, y_pred_proba, k=3):\n",
        "    \"\"\"\n",
        "    y_true: shape (n_samples,)\n",
        "    y_pred_proba: shape (n_samples, n_classes)\n",
        "    \"\"\"\n",
        "    top_k_preds = np.argsort(y_pred_proba, axis=1)[:, -k:]  # top-k class indices\n",
        "    correct = 0\n",
        "    for i in range(len(y_true)):\n",
        "        if y_true[i] in top_k_preds[i]:\n",
        "            correct += 1\n",
        "    return correct / len(y_true)\n",
        "\n",
        "# Example multiclass test\n",
        "y_true = np.array([0, 1, 2])\n",
        "y_pred_proba = np.array([\n",
        "    [0.7, 0.2, 0.1],\n",
        "    [0.1, 0.8, 0.1],\n",
        "    [0.2, 0.2, 0.6]\n",
        "])\n",
        "print(\"Top-1 Accuracy:\", top_k_accuracy(y_true, y_pred_proba, k=1))\n",
        "print(\"Top-2 Accuracy:\", top_k_accuracy(y_true, y_pred_proba, k=2))\n"
      ],
      "metadata": {
        "id": "WSbQ1JJ8RTZs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}