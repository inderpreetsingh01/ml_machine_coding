{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2PgRcmtWKMRlGA02iI1Aa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inderpreetsingh01/ml_machine_coding/blob/main/NLP_(BOW%2BTFIDF).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9_AqC9IQU5w7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "class BagOfWords:\n",
        "    def __init__(self):\n",
        "        self.vocab = {}\n",
        "        self.inverse_vocab = {}\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        # Simple tokenizer: lowercase + split on non-alphanumeric\n",
        "        tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
        "        return tokens\n",
        "\n",
        "    def fit(self, documents):\n",
        "        \"\"\"Build vocabulary from list of documents\"\"\"\n",
        "        vocab_set = set()\n",
        "        for doc in documents:\n",
        "            tokens = self._tokenize(doc)\n",
        "            vocab_set.update(tokens)\n",
        "\n",
        "        self.vocab = {word: idx for idx, word in enumerate(sorted(vocab_set))}\n",
        "        self.inverse_vocab = {idx: word for word, idx in self.vocab.items()}\n",
        "        return self\n",
        "\n",
        "    def transform(self, documents):\n",
        "        \"\"\"Transform docs into BoW vectors\"\"\"\n",
        "        rows = []\n",
        "        for doc in documents:\n",
        "            tokens = self._tokenize(doc)\n",
        "            vector = np.zeros(len(self.vocab), dtype=int)\n",
        "            for token in tokens:\n",
        "                if token in self.vocab:\n",
        "                    vector[self.vocab[token]] += 1\n",
        "            rows.append(vector)\n",
        "        return np.array(rows)\n",
        "\n",
        "    def fit_transform(self, documents):\n",
        "        \"\"\"Fit vocab + transform docs\"\"\"\n",
        "        self.fit(documents)\n",
        "        return self.transform(documents)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        "    \"Data science is fun\",\n",
        "    \"Machine learning is part of data science\",\n",
        "    \"Python makes machine learning easy\"\n",
        "]\n",
        "\n",
        "bow = BagOfWords()\n",
        "X = bow.fit_transform(docs)\n",
        "\n",
        "print(\"Vocabulary:\", bow.vocab)\n",
        "print(\"BoW Matrix:\\n\", X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBQw_I12VCJg",
        "outputId": "2368f7ba-0763-4f47-e442-2cdd75d16f14"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'data': 0, 'easy': 1, 'fun': 2, 'is': 3, 'learning': 4, 'machine': 5, 'makes': 6, 'of': 7, 'part': 8, 'python': 9, 'science': 10}\n",
            "BoW Matrix:\n",
            " [[1 0 1 1 0 0 0 0 0 0 1]\n",
            " [1 0 0 1 1 1 0 1 1 0 1]\n",
            " [0 1 0 0 1 1 1 0 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1V4WhL6pVD05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VfmwAHH5VUwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VQ9R16RaVUud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TfIDF"
      ],
      "metadata": {
        "id": "x1Wt4jEgVUsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "\n",
        "class TFIDFVectorizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = {}\n",
        "        self.idf = None\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
        "\n",
        "    def fit(self, documents):\n",
        "        \"\"\"Build vocabulary & compute IDF\"\"\"\n",
        "        vocab_set = set()\n",
        "        tokenized_docs = [self._tokenize(doc) for doc in documents]\n",
        "\n",
        "        # Build vocab\n",
        "        for tokens in tokenized_docs:\n",
        "            vocab_set.update(tokens)\n",
        "        self.vocab = {word: idx for idx, word in enumerate(sorted(vocab_set))}\n",
        "\n",
        "        # Compute IDF\n",
        "        N = len(documents)\n",
        "        df = np.zeros(len(self.vocab))\n",
        "\n",
        "        for tokens in tokenized_docs:\n",
        "            unique_tokens = set(tokens)\n",
        "            for token in unique_tokens:\n",
        "                df[self.vocab[token]] += 1\n",
        "\n",
        "        # idf = log((N+1)/(df+1)) + 1 for smoothing\n",
        "        self.idf = np.log((N + 1) / (df + 1)) + 1\n",
        "        return self\n",
        "\n",
        "    def transform(self, documents):\n",
        "        \"\"\"Transform docs into TF-IDF vectors\"\"\"\n",
        "        tfidf_matrix = []\n",
        "\n",
        "        for doc in documents:\n",
        "            tokens = self._tokenize(doc)\n",
        "            vec = np.zeros(len(self.vocab))\n",
        "\n",
        "            # Term frequency (raw count)\n",
        "            for token in tokens:\n",
        "                if token in self.vocab:\n",
        "                    vec[self.vocab[token]] += 1\n",
        "\n",
        "            # Normalize TF by doc length\n",
        "            if len(tokens) > 0:\n",
        "                vec = vec / len(tokens)\n",
        "\n",
        "            # Multiply by IDF\n",
        "            vec = vec * self.idf\n",
        "            tfidf_matrix.append(vec)\n",
        "\n",
        "        return np.array(tfidf_matrix)\n",
        "\n",
        "    def fit_transform(self, documents):\n",
        "        self.fit(documents)\n",
        "        return self.transform(documents)"
      ],
      "metadata": {
        "id": "B7vc6IzVVUqR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        "    \"Data science is fun\",\n",
        "    \"Machine learning is part of data science\",\n",
        "    \"Python makes machine learning easy\"\n",
        "]\n",
        "\n",
        "tfidf = TFIDFVectorizer()\n",
        "X = tfidf.fit_transform(docs)\n",
        "\n",
        "print(\"Vocabulary:\", tfidf.vocab)\n",
        "print(\"IDF:\", np.round(tfidf.idf, 3))\n",
        "print(\"TF-IDF Matrix:\\n\", np.round(X, 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO0fUvsZVUn9",
        "outputId": "c979e643-a1b5-4b02-82d8-3147bcb12ad6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'data': 0, 'easy': 1, 'fun': 2, 'is': 3, 'learning': 4, 'machine': 5, 'makes': 6, 'of': 7, 'part': 8, 'python': 9, 'science': 10}\n",
            "IDF: [1.288 1.693 1.693 1.288 1.288 1.288 1.693 1.693 1.693 1.693 1.288]\n",
            "TF-IDF Matrix:\n",
            " [[0.322 0.    0.423 0.322 0.    0.    0.    0.    0.    0.    0.322]\n",
            " [0.184 0.    0.    0.184 0.184 0.184 0.    0.242 0.242 0.    0.184]\n",
            " [0.    0.339 0.    0.    0.258 0.258 0.339 0.    0.    0.339 0.   ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "chysCsCzVZb2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}