{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMn9P/h3tm9a/FCC4fWmwZl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inderpreetsingh01/ml_machine_coding/blob/main/Linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Method  |  Time Complexity\n",
        "# Closed-form\tO(n³) => due to matrix inversion\n",
        "# Gradient Descent\t=> O(n_samples × n_features × n_iters)"
      ],
      "metadata": {
        "id": "SOyPk1cJKOG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1n26RBfBJ_xu",
        "outputId": "ce8769d8-8468-4aeb-99e1-3e566f846553"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Testing Closed-Form Solution ====\n",
            "Weights: [3.16933339 2.17747302]\n",
            "Bias: 4.886136132050842\n",
            "Metrics: {'mse': np.float64(0.24534574806972081), 'r2_score': np.float64(0.8375977028740942)}\n",
            "\n",
            "==== Testing Gradient Descent Solution ====\n",
            "Weights: [3.16909262 2.17727327]\n",
            "Bias: 4.8863618600794965\n",
            "Metrics: {'mse': np.float64(0.24534575648891244), 'r2_score': np.float64(0.8375976973011586)}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class LinearRegression:\n",
        "    \"\"\"\n",
        "    Linear Regression Model supporting both:\n",
        "    - Closed-form solution (Normal Equation)\n",
        "    - Gradient Descent Optimization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, method=\"gradient_descent\", lr=0.01, n_iters=1000):\n",
        "        self.method = method\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model using selected method.\n",
        "        \"\"\"\n",
        "        if self.method == \"closed_form\":\n",
        "            self._fit_closed_form(X, y)\n",
        "        elif self.method == \"gradient_descent\":\n",
        "            self._fit_gradient_descent(X, y)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown method selected\")\n",
        "\n",
        "    def _fit_closed_form(self, X, y):\n",
        "        \"\"\"\n",
        "        Closed-form solution using Normal Equation:\n",
        "        theta = (X^T X)^(-1) X^T y\n",
        "        \"\"\"\n",
        "        # Add bias term to features (intercept)\n",
        "        X_bias = np.c_[np.ones((X.shape[0], 1)), X]  # shape (n_samples, n_features + 1)\n",
        "        theta_best = np.linalg.inv(X_bias.T @ X_bias) @ X_bias.T @ y\n",
        "        self.bias = theta_best[0]\n",
        "        self.weights = theta_best[1:]\n",
        "\n",
        "    def _fit_gradient_descent(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit using Batch Gradient Descent.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            y_pred = np.dot(X, self.weights) + self.bias\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
        "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict target values.\n",
        "        \"\"\"\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "    def evaluate(self, X, y_true):\n",
        "        \"\"\"\n",
        "        Evaluate using Mean Squared Error and R^2 Score.\n",
        "        \"\"\"\n",
        "        y_pred = self.predict(X)\n",
        "        mse = np.mean((y_true - y_pred) ** 2)\n",
        "        r2 = 1 - (np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2))\n",
        "        return {\"mse\": mse, \"r2_score\": r2}\n",
        "\n",
        "\n",
        "# ==== Synthetic Test ====\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 2)\n",
        "true_weights = np.array([3, 2])\n",
        "y = X @ true_weights + 5 + np.random.randn(100) * 0.5  # y = 3*x1 + 2*x2 + 5 + noise\n",
        "\n",
        "print(\"==== Testing Closed-Form Solution ====\")\n",
        "model_closed = LinearRegression(method=\"closed_form\")\n",
        "model_closed.fit(X, y)\n",
        "metrics_closed = model_closed.evaluate(X, y)\n",
        "print(\"Weights:\", model_closed.weights)\n",
        "print(\"Bias:\", model_closed.bias)\n",
        "print(\"Metrics:\", metrics_closed)\n",
        "\n",
        "print(\"\\n==== Testing Gradient Descent Solution ====\")\n",
        "model_gd = LinearRegression(method=\"gradient_descent\", lr=0.1, n_iters=1000)\n",
        "model_gd.fit(X, y)\n",
        "metrics_gd = model_gd.evaluate(X, y)\n",
        "print(\"Weights:\", model_gd.weights)\n",
        "print(\"Bias:\", model_gd.bias)\n",
        "print(\"Metrics:\", metrics_gd)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uszazAkfKIP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h0Od8IVdKxGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aaaeSu_pKxFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UeBV0wd6KxAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with L1 and L2 regularization\n",
        "class LinearRegression:\n",
        "    \"\"\"\n",
        "    Linear Regression supporting:\n",
        "    - Closed-form (Normal Equation) [with L2 Regularization (Ridge) only]\n",
        "    - Gradient Descent [with L1 (Lasso) and L2 (Ridge) Regularization]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 method=\"gradient_descent\",\n",
        "                 lr=0.01,\n",
        "                 n_iters=1000,\n",
        "                 l1_lambda=0.0,\n",
        "                 l2_lambda=0.0):\n",
        "        self.method = method\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.l1_lambda = l1_lambda\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        if self.method == \"closed_form\":\n",
        "            if self.l1_lambda != 0:\n",
        "                raise NotImplementedError(\"Closed-form solution doesn't support L1 regularization.\")\n",
        "            self._fit_closed_form(X, y)\n",
        "        elif self.method == \"gradient_descent\":\n",
        "            self._fit_gradient_descent(X, y)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown method selected\")\n",
        "\n",
        "    def _fit_closed_form(self, X, y):\n",
        "        \"\"\"\n",
        "        Closed-form solution with L2 Regularization:\n",
        "        theta = (X^T X + λ * I)^(-1) X^T y\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        X_bias = np.c_[np.ones((n_samples, 1)), X]\n",
        "        I = np.eye(n_features + 1)\n",
        "        I[0, 0] = 0  # Don't regularize bias term\n",
        "\n",
        "        theta_best = np.linalg.inv(\n",
        "            X_bias.T @ X_bias + self.l2_lambda * I\n",
        "        ) @ X_bias.T @ y\n",
        "\n",
        "        self.bias = theta_best[0]\n",
        "        self.weights = theta_best[1:]\n",
        "\n",
        "    def _fit_gradient_descent(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            y_pred = np.dot(X, self.weights) + self.bias\n",
        "            error = y_pred - y\n",
        "\n",
        "            # Gradients for weights and bias\n",
        "            dw = (1 / n_samples) * np.dot(X.T, error)\n",
        "            db = (1 / n_samples) * np.sum(error)\n",
        "\n",
        "            # L2 Regularization\n",
        "            dw += self.l2_lambda * self.weights\n",
        "\n",
        "            # L1 Regularization (Sub-gradient)\n",
        "            dw += self.l1_lambda * np.sign(self.weights)\n",
        "\n",
        "            # Update parameters\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "    def evaluate(self, X, y_true):\n",
        "        y_pred = self.predict(X)\n",
        "        mse = np.mean((y_true - y_pred) ** 2)\n",
        "        r2 = 1 - np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "        return {\"mse\": mse, \"r2_score\": r2}\n",
        "\n",
        "\n",
        "# ==== Synthetic Test ====\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 2)\n",
        "true_weights = np.array([3, 2])\n",
        "y = X @ true_weights + 5 + np.random.randn(100) * 0.5\n",
        "\n",
        "print(\"==== Closed-Form Ridge Regression ====\")\n",
        "model_ridge = LinearRegression(method=\"closed_form\", l2_lambda=0.1)\n",
        "model_ridge.fit(X, y)\n",
        "metrics_ridge = model_ridge.evaluate(X, y)\n",
        "print(\"Weights:\", model_ridge.weights)\n",
        "print(\"Bias:\", model_ridge.bias)\n",
        "print(\"Metrics:\", metrics_ridge)\n",
        "\n",
        "print(\"\\n==== Gradient Descent with L1 + L2 Regularization ====\")\n",
        "model_l1_l2 = LinearRegression(method=\"gradient_descent\", lr=0.1, n_iters=1000, l1_lambda=0.1, l2_lambda=0.1)\n",
        "model_l1_l2.fit(X, y)\n",
        "metrics_l1_l2 = model_l1_l2.evaluate(X, y)\n",
        "print(\"Weights:\", model_l1_l2.weights)\n",
        "print(\"Bias:\", model_l1_l2.bias)\n",
        "print(\"Metrics:\", metrics_l1_l2)"
      ],
      "metadata": {
        "id": "D06xhiVYKu7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "meg-RUWhK1qL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X0AHVGvIK1of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Scaling (Standardization):\n",
        "# 1. Normalizes features to zero mean & unit variance.\n",
        "# 2. Handled automatically during fit and predict.\n",
        "# 3. Scaling params stored inside the model.\n",
        "\n",
        "# Early Stopping:\n",
        "# 1. Stops gradient descent early if MSE improvement < tol threshold.\n",
        "# 2. Saves training time, avoids overfitting on small datasets.\n",
        "\n",
        "class LinearRegression:\n",
        "    \"\"\"\n",
        "    Linear Regression with:\n",
        "    - Closed-form (with L2 Regularization)\n",
        "    - Gradient Descent (with L1/L2 Regularization)\n",
        "    - Feature Scaling (Standardization)\n",
        "    - Early Stopping\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 method=\"gradient_descent\",\n",
        "                 lr=0.01,\n",
        "                 n_iters=1000,\n",
        "                 l1_lambda=0.0,\n",
        "                 l2_lambda=0.0,\n",
        "                 early_stopping=False,\n",
        "                 tol=1e-4):\n",
        "        self.method = method\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.l1_lambda = l1_lambda\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.early_stopping = early_stopping\n",
        "        self.tol = tol\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.scaler_mean = None\n",
        "        self.scaler_std = None\n",
        "\n",
        "    def _scale_features(self, X, fit=True):\n",
        "        if fit:\n",
        "            self.scaler_mean = np.mean(X, axis=0)\n",
        "            self.scaler_std = np.std(X, axis=0)\n",
        "            self.scaler_std[self.scaler_std == 0] = 1  # Prevent division by zero\n",
        "        return (X - self.scaler_mean) / self.scaler_std\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X_scaled = self._scale_features(X, fit=True)\n",
        "        if self.method == \"closed_form\":\n",
        "            if self.l1_lambda != 0:\n",
        "                raise NotImplementedError(\"Closed-form does not support L1 regularization.\")\n",
        "            self._fit_closed_form(X_scaled, y)\n",
        "        elif self.method == \"gradient_descent\":\n",
        "            self._fit_gradient_descent(X_scaled, y)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown method selected\")\n",
        "\n",
        "    def _fit_closed_form(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        X_bias = np.c_[np.ones((n_samples, 1)), X]\n",
        "        I = np.eye(n_features + 1)\n",
        "        I[0, 0] = 0  # Don't regularize bias term\n",
        "\n",
        "        theta_best = np.linalg.inv(\n",
        "            X_bias.T @ X_bias + self.l2_lambda * I\n",
        "        ) @ X_bias.T @ y\n",
        "\n",
        "        self.bias = theta_best[0]\n",
        "        self.weights = theta_best[1:]\n",
        "\n",
        "    def _fit_gradient_descent(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        prev_loss = float('inf')\n",
        "\n",
        "        for i in range(self.n_iters):\n",
        "            y_pred = np.dot(X, self.weights) + self.bias\n",
        "            error = y_pred - y\n",
        "\n",
        "            dw = (1 / n_samples) * np.dot(X.T, error)\n",
        "            db = (1 / n_samples) * np.sum(error)\n",
        "\n",
        "            # Regularization\n",
        "            dw += self.l2_lambda * self.weights\n",
        "            dw += self.l1_lambda * np.sign(self.weights)\n",
        "\n",
        "            # Parameter update\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "            # Early stopping\n",
        "            current_loss = np.mean(error ** 2)\n",
        "            if self.early_stopping and abs(prev_loss - current_loss) < self.tol:\n",
        "                print(f\"Early stopping at iteration {i+1}. Loss change: {abs(prev_loss - current_loss):.6f}\")\n",
        "                break\n",
        "            prev_loss = current_loss\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_scaled = self._scale_features(X, fit=False)\n",
        "        return np.dot(X_scaled, self.weights) + self.bias\n",
        "\n",
        "    def evaluate(self, X, y_true):\n",
        "        y_pred = self.predict(X)\n",
        "        mse = np.mean((y_true - y_pred) ** 2)\n",
        "        r2 = 1 - np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "        return {\"mse\": mse, \"r2_score\": r2}\n",
        "\n",
        "\n",
        "# ==== Synthetic Test ====\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 2)\n",
        "true_weights = np.array([3, 2])\n",
        "y = X @ true_weights + 5 + np.random.randn(100) * 0.5\n",
        "\n",
        "print(\"==== Gradient Descent with Feature Scaling, L1/L2 Regularization, Early Stopping ====\")\n",
        "model = LinearRegression(\n",
        "    method=\"gradient_descent\",\n",
        "    lr=0.1,\n",
        "    n_iters=1000,\n",
        "    l1_lambda=0.1,\n",
        "    l2_lambda=0.1,\n",
        "    early_stopping=True,\n",
        "    tol=1e-5\n",
        ")\n",
        "model.fit(X, y)\n",
        "metrics = model.evaluate(X, y)\n",
        "print(\"Weights:\", model.weights)\n",
        "print(\"Bias:\", model.bias)\n",
        "print(\"Metrics:\", metrics)"
      ],
      "metadata": {
        "id": "4JGUYu45K1ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sAbLQ0XMK1kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yzT1yOoyK1jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Added learning rate decay inside gradient descent.\n",
        "# Decays after every iteration based on:\n",
        "class LinearRegression:\n",
        "    \"\"\"\n",
        "    Linear Regression with:\n",
        "    - Closed-form (with L2 Regularization)\n",
        "    - Gradient Descent (with L1/L2 Regularization)\n",
        "    - Feature Scaling (Standardization)\n",
        "    - Early Stopping\n",
        "    - Learning Rate Decay\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 method=\"gradient_descent\",\n",
        "                 lr=0.01,\n",
        "                 n_iters=1000,\n",
        "                 l1_lambda=0.0,\n",
        "                 l2_lambda=0.0,\n",
        "                 early_stopping=False,\n",
        "                 tol=1e-4,\n",
        "                 decay_rate=0.0):\n",
        "        self.method = method\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.l1_lambda = l1_lambda\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.early_stopping = early_stopping\n",
        "        self.tol = tol\n",
        "        self.decay_rate = decay_rate\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.scaler_mean = None\n",
        "        self.scaler_std = None\n",
        "\n",
        "    def _scale_features(self, X, fit=True):\n",
        "        if fit:\n",
        "            self.scaler_mean = np.mean(X, axis=0)\n",
        "            self.scaler_std = np.std(X, axis=0)\n",
        "            self.scaler_std[self.scaler_std == 0] = 1\n",
        "        return (X - self.scaler_mean) / self.scaler_std\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X_scaled = self._scale_features(X, fit=True)\n",
        "        if self.method == \"closed_form\":\n",
        "            if self.l1_lambda != 0:\n",
        "                raise NotImplementedError(\"Closed-form does not support L1 regularization.\")\n",
        "            self._fit_closed_form(X_scaled, y)\n",
        "        elif self.method == \"gradient_descent\":\n",
        "            self._fit_gradient_descent(X_scaled, y)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown method selected\")\n",
        "\n",
        "    def _fit_closed_form(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        X_bias = np.c_[np.ones((n_samples, 1)), X]\n",
        "        I = np.eye(n_features + 1)\n",
        "        I[0, 0] = 0\n",
        "\n",
        "        theta_best = np.linalg.inv(\n",
        "            X_bias.T @ X_bias + self.l2_lambda * I\n",
        "        ) @ X_bias.T @ y\n",
        "\n",
        "        self.bias = theta_best[0]\n",
        "        self.weights = theta_best[1:]\n",
        "\n",
        "    def _fit_gradient_descent(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        prev_loss = float('inf')\n",
        "        current_lr = self.lr\n",
        "\n",
        "        for i in range(self.n_iters):\n",
        "            y_pred = np.dot(X, self.weights) + self.bias\n",
        "            error = y_pred - y\n",
        "\n",
        "            dw = (1 / n_samples) * np.dot(X.T, error)\n",
        "            db = (1 / n_samples) * np.sum(error)\n",
        "\n",
        "            dw += self.l2_lambda * self.weights\n",
        "            dw += self.l1_lambda * np.sign(self.weights)\n",
        "\n",
        "            self.weights -= current_lr * dw\n",
        "            self.bias -= current_lr * db\n",
        "\n",
        "            current_loss = np.mean(error ** 2)\n",
        "            if self.early_stopping and abs(prev_loss - current_loss) < self.tol:\n",
        "                print(f\"Early stopping at iteration {i+1}. Loss change: {abs(prev_loss - current_loss):.6f}\")\n",
        "                break\n",
        "            prev_loss = current_loss\n",
        "\n",
        "            # Learning rate decay step\n",
        "            if self.decay_rate > 0:\n",
        "                current_lr = self.lr / (1 + self.decay_rate * (i + 1))\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_scaled = self._scale_features(X, fit=False)\n",
        "        return np.dot(X_scaled, self.weights) + self.bias\n",
        "\n",
        "    def evaluate(self, X, y_true):\n",
        "        y_pred = self.predict(X)\n",
        "        mse = np.mean((y_true - y_pred) ** 2)\n",
        "        r2 = 1 - np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "        return {\"mse\": mse, \"r2_score\": r2}\n",
        "\n",
        "\n",
        "# ==== Synthetic Test ====\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 2)\n",
        "true_weights = np.array([3, 2])\n",
        "y = X @ true_weights + 5 + np.random.randn(100) * 0.5\n",
        "\n",
        "print(\"==== Gradient Descent with L1/L2 Regularization, Early Stopping, Learning Rate Decay ====\")\n",
        "model = LinearRegression(\n",
        "    method=\"gradient_descent\",\n",
        "    lr=0.1,\n",
        "    n_iters=1000,\n",
        "    l1_lambda=0.1,\n",
        "    l2_lambda=0.1,\n",
        "    early_stopping=True,\n",
        "    tol=1e-5,\n",
        "    decay_rate=0.05\n",
        ")\n",
        "model.fit(X, y)\n",
        "metrics = model.evaluate(X, y)\n",
        "print(\"Weights:\", model.weights)\n",
        "print(\"Bias:\", model.bias)\n",
        "print(\"Metrics:\", metrics)"
      ],
      "metadata": {
        "id": "vb_Rv_aTK1f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature\tBenefit\n",
        "# Feature Scaling\tStable, fast convergence\n",
        "# L1 & L2 Regularization\tSparsity + Generalization\n",
        "# Early Stopping\tSaves compute, prevents overfitting\n",
        "# Learning Rate Decay\tSmooth convergence, avoids overshooting\n",
        "# Modular Class Design\tReusable for real-world systems and machine coding"
      ],
      "metadata": {
        "id": "4Pj_tAgCK1ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dh6EGe-r7PUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_linear_regression():\n",
        "    np.random.seed(42)\n",
        "    # Generate synthetic data: y = 3*x1 + 2*x2 + 5 + noise\n",
        "    X = np.random.rand(100, 2)\n",
        "    true_weights = np.array([3, 2])\n",
        "    y = X @ true_weights + 5 + np.random.randn(100) * 0.5  # with noise\n",
        "\n",
        "    model = LinearRegression(\n",
        "        method=\"gradient_descent\",\n",
        "        lr=0.1,\n",
        "        n_iters=1000,\n",
        "        l1_lambda=0.1,\n",
        "        l2_lambda=0.1,\n",
        "        early_stopping=True,\n",
        "        tol=1e-5,\n",
        "        decay_rate=0.05\n",
        "    )\n",
        "    model.fit(X, y)\n",
        "    metrics = model.evaluate(X, y)\n",
        "\n",
        "    # ✅ Test assertions\n",
        "    assert metrics['mse'] < 0.5, f\"Expected low MSE, got {metrics['mse']}\"\n",
        "    assert 0.95 < metrics['r2_score'] <= 1.0, f\"Expected high R2 score, got {metrics['r2_score']}\"\n",
        "    print(\"✅ Linear Regression Test Passed:\", metrics)"
      ],
      "metadata": {
        "id": "3LYBK65E7PxA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}