{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQhj5Og2DP+EbxbFLXMx3b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inderpreetsingh01/ml_machine_coding/blob/main/Word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1f4TKxcmVg5-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "class Word2Vec:\n",
        "    def __init__(self, vocab_size, embed_size=50, window=2, negative_samples=5, lr=0.01, epochs=10):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.window = window\n",
        "        self.negative_samples = negative_samples\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "\n",
        "        # Initialize weights\n",
        "        self.W1 = np.random.randn(vocab_size, embed_size) * 0.01  # input -> hidden\n",
        "        self.W2 = np.random.randn(embed_size, vocab_size) * 0.01  # hidden -> output\n",
        "\n",
        "    def _softmax(self, x):\n",
        "        e_x = np.exp(x - np.max(x))\n",
        "        return e_x / e_x.sum(axis=0)\n",
        "\n",
        "    def generate_training_data(self, corpus, word_to_idx):\n",
        "        \"\"\"Create skip-gram pairs with window size\"\"\"\n",
        "        training_pairs = []\n",
        "        for sentence in corpus:\n",
        "            for i, word in enumerate(sentence):\n",
        "                center = word_to_idx[word]\n",
        "                context = list(range(max(0, i - self.window), min(len(sentence), i + self.window + 1)))\n",
        "                context.remove(i)\n",
        "                for j in context:\n",
        "                    context_word = word_to_idx[sentence[j]]\n",
        "                    training_pairs.append((center, context_word))\n",
        "        return training_pairs\n",
        "\n",
        "    def train(self, training_pairs):\n",
        "        for epoch in range(self.epochs):\n",
        "            loss = 0\n",
        "            for center, context in training_pairs:\n",
        "                # Forward pass\n",
        "                h = self.W1[center]                  # hidden layer\n",
        "                u = np.dot(h, self.W2)               # scores for all words\n",
        "                y_pred = self._softmax(u)            # predicted prob distribution\n",
        "\n",
        "                # True output\n",
        "                y_true = np.zeros(self.vocab_size)\n",
        "                y_true[context] = 1.0\n",
        "\n",
        "                # Compute error\n",
        "                e = y_pred - y_true\n",
        "                loss += -np.log(y_pred[context] + 1e-9)\n",
        "\n",
        "                # Backpropagation\n",
        "                dW2 = np.outer(h, e)\n",
        "                dW1 = np.dot(self.W2, e)\n",
        "\n",
        "                # Update weights\n",
        "                self.W1[center] -= self.lr * dW1\n",
        "                self.W2 -= self.lr * dW2\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{self.epochs}, Loss={loss:.4f}\")\n",
        "\n",
        "    def get_vector(self, word, word_to_idx):\n",
        "        return self.W1[word_to_idx[word]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example corpus\n",
        "corpus = [\n",
        "    \"data science is fun\",\n",
        "    \"machine learning is part of data science\",\n",
        "    \"python makes machine learning easy\"\n",
        "]\n",
        "\n",
        "# Preprocess\n",
        "corpus = [re.findall(r\"\\b\\w+\\b\", sentence.lower()) for sentence in corpus]\n",
        "words = [word for sentence in corpus for word in sentence]\n",
        "vocab = sorted(set(words))\n",
        "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
        "\n",
        "# Train data\n",
        "w2v = Word2Vec(vocab_size=len(vocab), embed_size=10, window=2, lr=0.05, epochs=50)\n",
        "training_pairs = w2v.generate_training_data(corpus, word_to_idx)\n",
        "w2v.train(training_pairs)\n",
        "\n",
        "# Get embeddings\n",
        "print(\"Embedding for 'data':\", w2v.get_vector(\"data\", word_to_idx))\n",
        "print(\"Embedding for 'science':\", w2v.get_vector(\"science\", word_to_idx))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTAr5vakVtOV",
        "outputId": "f7fd94c1-1f71-4c73-835f-67d22447688f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss=110.3050\n",
            "Epoch 2/50, Loss=110.3009\n",
            "Epoch 3/50, Loss=110.2965\n",
            "Epoch 4/50, Loss=110.2917\n",
            "Epoch 5/50, Loss=110.2860\n",
            "Epoch 6/50, Loss=110.2789\n",
            "Epoch 7/50, Loss=110.2698\n",
            "Epoch 8/50, Loss=110.2580\n",
            "Epoch 9/50, Loss=110.2425\n",
            "Epoch 10/50, Loss=110.2218\n",
            "Epoch 11/50, Loss=110.1942\n",
            "Epoch 12/50, Loss=110.1570\n",
            "Epoch 13/50, Loss=110.1069\n",
            "Epoch 14/50, Loss=110.0393\n",
            "Epoch 15/50, Loss=109.9480\n",
            "Epoch 16/50, Loss=109.8248\n",
            "Epoch 17/50, Loss=109.6590\n",
            "Epoch 18/50, Loss=109.4364\n",
            "Epoch 19/50, Loss=109.1396\n",
            "Epoch 20/50, Loss=108.7470\n",
            "Epoch 21/50, Loss=108.2338\n",
            "Epoch 22/50, Loss=107.5740\n",
            "Epoch 23/50, Loss=106.7438\n",
            "Epoch 24/50, Loss=105.7280\n",
            "Epoch 25/50, Loss=104.5271\n",
            "Epoch 26/50, Loss=103.1647\n",
            "Epoch 27/50, Loss=101.6883\n",
            "Epoch 28/50, Loss=100.1627\n",
            "Epoch 29/50, Loss=98.6558\n",
            "Epoch 30/50, Loss=97.2221\n",
            "Epoch 31/50, Loss=95.8937\n",
            "Epoch 32/50, Loss=94.6794\n",
            "Epoch 33/50, Loss=93.5715\n",
            "Epoch 34/50, Loss=92.5536\n",
            "Epoch 35/50, Loss=91.6070\n",
            "Epoch 36/50, Loss=90.7145\n",
            "Epoch 37/50, Loss=89.8619\n",
            "Epoch 38/50, Loss=89.0390\n",
            "Epoch 39/50, Loss=88.2396\n",
            "Epoch 40/50, Loss=87.4610\n",
            "Epoch 41/50, Loss=86.7031\n",
            "Epoch 42/50, Loss=85.9672\n",
            "Epoch 43/50, Loss=85.2545\n",
            "Epoch 44/50, Loss=84.5656\n",
            "Epoch 45/50, Loss=83.9001\n",
            "Epoch 46/50, Loss=83.2571\n",
            "Epoch 47/50, Loss=82.6357\n",
            "Epoch 48/50, Loss=82.0357\n",
            "Epoch 49/50, Loss=81.4582\n",
            "Epoch 50/50, Loss=80.9053\n",
            "Embedding for 'data': [-0.04701352  0.01301214 -0.76937524  0.52168052  0.02482613 -0.2801485\n",
            " -1.1245669   0.01036015  0.86288544 -0.03220867]\n",
            "Embedding for 'science': [ 0.2696942   0.44766678  0.1920752   0.20004388  0.53856414 -0.30768589\n",
            " -1.26916483 -0.13530262 -0.82491178  0.33937152]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TP7hOdA-Vv00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jya9pX9yV-C5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with negative sampling"
      ],
      "metadata": {
        "id": "fzOg44I7V-Bh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "class Word2VecNS:\n",
        "    def __init__(self, vocab_size, embed_size=50, window=2, negative_samples=5, lr=0.01, epochs=10):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.window = window\n",
        "        self.negative_samples = negative_samples\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "\n",
        "        # Initialize embeddings\n",
        "        self.W1 = np.random.randn(vocab_size, embed_size) * 0.01  # input -> hidden\n",
        "        self.W2 = np.random.randn(embed_size, vocab_size) * 0.01  # hidden -> output\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def generate_training_data(self, corpus, word_to_idx):\n",
        "        \"\"\"Generate skip-gram (center, context) pairs\"\"\"\n",
        "        pairs = []\n",
        "        for sentence in corpus:\n",
        "            for i, word in enumerate(sentence):\n",
        "                center = word_to_idx[word]\n",
        "                context_range = range(max(0, i - self.window), min(len(sentence), i + self.window + 1))\n",
        "                for j in context_range:\n",
        "                    if i != j:\n",
        "                        context = word_to_idx[sentence[j]]\n",
        "                        pairs.append((center, context))\n",
        "        return pairs\n",
        "\n",
        "    def _get_negative_samples(self, true_idx):\n",
        "        \"\"\"Sample negative words (not the true context word)\"\"\"\n",
        "        negatives = []\n",
        "        while len(negatives) < self.negative_samples:\n",
        "            neg = random.randint(0, self.vocab_size - 1)\n",
        "            if neg != true_idx:\n",
        "                negatives.append(neg)\n",
        "        return negatives\n",
        "\n",
        "    def train(self, training_pairs):\n",
        "        for epoch in range(self.epochs):\n",
        "            total_loss = 0\n",
        "            for center, context in training_pairs:\n",
        "                h = self.W1[center]   # hidden layer representation\n",
        "\n",
        "                # Positive sample (true context)\n",
        "                u_pos = np.dot(h, self.W2[:, context])\n",
        "                p_pos = self._sigmoid(u_pos)\n",
        "                loss_pos = -np.log(p_pos + 1e-9)\n",
        "\n",
        "                # Gradient update for positive sample\n",
        "                grad_pos = self.lr * (1 - p_pos)\n",
        "                self.W1[center] += grad_pos * self.W2[:, context]\n",
        "                self.W2[:, context] += grad_pos * h\n",
        "\n",
        "                # Negative samples\n",
        "                negatives = self._get_negative_samples(context)\n",
        "                loss_neg = 0\n",
        "                for neg in negatives:\n",
        "                    u_neg = np.dot(h, self.W2[:, neg])\n",
        "                    p_neg = self._sigmoid(-u_neg)  # target=0\n",
        "                    loss_neg += -np.log(p_neg + 1e-9)\n",
        "\n",
        "                    grad_neg = self.lr * (0 - self._sigmoid(u_neg))\n",
        "                    self.W1[center] += grad_neg * self.W2[:, neg]\n",
        "                    self.W2[:, neg] += grad_neg * h\n",
        "\n",
        "                total_loss += (loss_pos + loss_neg)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{self.epochs}, Loss={total_loss:.4f}\")\n",
        "\n",
        "    def get_vector(self, word, word_to_idx):\n",
        "        return self.W1[word_to_idx[word]]\n",
        "\n",
        "    def similarity(self, word1, word2, word_to_idx):\n",
        "        v1 = self.get_vector(word1, word_to_idx)\n",
        "        v2 = self.get_vector(word2, word_to_idx)\n",
        "        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
      ],
      "metadata": {
        "id": "R9LWRuvKV9-6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example corpus\n",
        "corpus = [\n",
        "    \"data science is fun\",\n",
        "    \"machine learning is part of data science\",\n",
        "    \"python makes machine learning easy\"\n",
        "]\n",
        "\n",
        "# Preprocess\n",
        "corpus = [re.findall(r\"\\b\\w+\\b\", sentence.lower()) for sentence in corpus]\n",
        "words = [word for sentence in corpus for word in sentence]\n",
        "vocab = sorted(set(words))\n",
        "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
        "\n",
        "# Train\n",
        "w2v = Word2VecNS(vocab_size=len(vocab), embed_size=10, window=2, negative_samples=3, lr=0.05, epochs=50)\n",
        "training_pairs = w2v.generate_training_data(corpus, word_to_idx)\n",
        "w2v.train(training_pairs)\n",
        "\n",
        "# Embeddings & similarity\n",
        "print(\"Vector for 'data':\", w2v.get_vector(\"data\", word_to_idx))\n",
        "print(\"Similarity(data, science):\", w2v.similarity(\"data\", \"science\", word_to_idx))\n",
        "print(\"Similarity(machine, python):\", w2v.similarity(\"machine\", \"python\", word_to_idx))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwfL11-vV989",
        "outputId": "c3a0db86-09b6-4553-8670-2a82afa53b94"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss=127.5374\n",
            "Epoch 2/50, Loss=127.5320\n",
            "Epoch 3/50, Loss=127.5233\n",
            "Epoch 4/50, Loss=127.5142\n",
            "Epoch 5/50, Loss=127.5028\n",
            "Epoch 6/50, Loss=127.4939\n",
            "Epoch 7/50, Loss=127.4603\n",
            "Epoch 8/50, Loss=127.4186\n",
            "Epoch 9/50, Loss=127.3511\n",
            "Epoch 10/50, Loss=127.2475\n",
            "Epoch 11/50, Loss=127.0989\n",
            "Epoch 12/50, Loss=126.8821\n",
            "Epoch 13/50, Loss=126.5178\n",
            "Epoch 14/50, Loss=125.8956\n",
            "Epoch 15/50, Loss=125.1592\n",
            "Epoch 16/50, Loss=123.8950\n",
            "Epoch 17/50, Loss=121.9493\n",
            "Epoch 18/50, Loss=119.7874\n",
            "Epoch 19/50, Loss=116.9568\n",
            "Epoch 20/50, Loss=114.4596\n",
            "Epoch 21/50, Loss=110.9105\n",
            "Epoch 22/50, Loss=108.1127\n",
            "Epoch 23/50, Loss=105.3525\n",
            "Epoch 24/50, Loss=103.6663\n",
            "Epoch 25/50, Loss=102.7386\n",
            "Epoch 26/50, Loss=101.4306\n",
            "Epoch 27/50, Loss=100.0268\n",
            "Epoch 28/50, Loss=98.4538\n",
            "Epoch 29/50, Loss=98.5548\n",
            "Epoch 30/50, Loss=99.5451\n",
            "Epoch 31/50, Loss=97.3730\n",
            "Epoch 32/50, Loss=97.0366\n",
            "Epoch 33/50, Loss=94.6951\n",
            "Epoch 34/50, Loss=95.3362\n",
            "Epoch 35/50, Loss=94.5077\n",
            "Epoch 36/50, Loss=92.3070\n",
            "Epoch 37/50, Loss=88.9826\n",
            "Epoch 38/50, Loss=90.4329\n",
            "Epoch 39/50, Loss=90.2482\n",
            "Epoch 40/50, Loss=87.9708\n",
            "Epoch 41/50, Loss=84.5147\n",
            "Epoch 42/50, Loss=92.1372\n",
            "Epoch 43/50, Loss=89.3213\n",
            "Epoch 44/50, Loss=85.8734\n",
            "Epoch 45/50, Loss=81.6621\n",
            "Epoch 46/50, Loss=81.2258\n",
            "Epoch 47/50, Loss=85.0723\n",
            "Epoch 48/50, Loss=80.2077\n",
            "Epoch 49/50, Loss=80.1244\n",
            "Epoch 50/50, Loss=83.2239\n",
            "Vector for 'data': [-1.07944711 -0.30922856 -0.46407585  0.92056471  0.37451453  0.18098281\n",
            "  0.28267882 -0.81490255  0.65158077 -0.73180477]\n",
            "Similarity(data, science): 0.5708940526585481\n",
            "Similarity(machine, python): 0.7530059228379716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qq1N48xqWCJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JKNBt1DZWDTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AtSJ4h2fWECx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mJ_JTjeUWEAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T3vTAf0ZWD-D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}