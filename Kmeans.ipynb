{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNff8PiNtP3TXbl+l+SXIXb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inderpreetsingh01/ml_machine_coding/blob/main/Kmeans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handles empty clusters gracefully (retains old centroid)\n",
        "# Supports convergence tolerance and max iterations\n",
        "# Easy to extend to k-means++ or mini-batch k-means"
      ],
      "metadata": {
        "id": "93lWHF9RLasq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9adPcVcLNxb"
      },
      "outputs": [],
      "source": [
        "class KMeans:\n",
        "    def __init__(self, k=3, max_iters=100, tol=1e-4, random_state=None):\n",
        "        self.k = k\n",
        "        self.max_iters = max_iters\n",
        "        self.tol = tol\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X):\n",
        "        np.random.seed(self.random_state)\n",
        "        n_samples, _ = X.shape\n",
        "\n",
        "        # Step 1: Initialize centroids\n",
        "        random_indices = np.random.choice(n_samples, self.k, replace=False)\n",
        "        self.centroids = X[random_indices]\n",
        "\n",
        "        for _ in range(self.max_iters):\n",
        "            # Step 2: Assign to nearest centroid\n",
        "            distances = self._compute_distances(X, self.centroids)\n",
        "            self.labels = np.argmin(distances, axis=1)\n",
        "\n",
        "            # Step 3: Update centroids\n",
        "            new_centroids = np.array([\n",
        "                X[self.labels == i].mean(axis=0) if np.any(self.labels == i) else self.centroids[i]\n",
        "                for i in range(self.k)\n",
        "            ])\n",
        "\n",
        "            # Step 4: Check for convergence\n",
        "            shift = np.linalg.norm(self.centroids - new_centroids)\n",
        "            self.centroids = new_centroids\n",
        "            if shift < self.tol:\n",
        "                break\n",
        "\n",
        "        # Store final inertia\n",
        "        self._inertia = self._compute_inertia(X)\n",
        "\n",
        "    def _compute_distances(self, X, centroids):\n",
        "        return np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
        "\n",
        "    def _compute_inertia(self, X):\n",
        "        return np.sum((np.linalg.norm(X - self.centroids[self.labels], axis=1)) ** 2)\n",
        "\n",
        "    def inertia(self):\n",
        "        return self._inertia\n",
        "\n",
        "    def predict(self, X):\n",
        "        distances = self._compute_distances(X, self.centroids)\n",
        "        return np.argmin(distances, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
        "\n",
        "model = KMeans(k=4, random_state=42)\n",
        "model.fit(X)\n",
        "\n",
        "print(\"Inertia:\", model.inertia())\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=model.labels, cmap='viridis')\n",
        "plt.scatter(model.centroids[:, 0], model.centroids[:, 1], c='red', s=200, marker='X')\n",
        "plt.title(\"K-Means Clustering\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dlRkeMGfLVlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DB1j60yQLVji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# This version is O(n²) due to pairwise distance matrix — fine for ≤10k points.\n",
        "def silhouette_score(X, labels):\n",
        "    n = len(X)\n",
        "    unique_labels = np.unique(labels)\n",
        "    k = len(unique_labels)\n",
        "    if k == 1 or k == n:\n",
        "        return 0  # Not meaningful\n",
        "\n",
        "    # Precompute pairwise distances\n",
        "    dist_matrix = np.linalg.norm(X[:, np.newaxis] - X, axis=2)\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    for i in range(n):\n",
        "        same_cluster = (labels == labels[i])\n",
        "        other_clusters = unique_labels[unique_labels != labels[i]]\n",
        "\n",
        "        # a(i): mean intra-cluster distance\n",
        "        a = np.mean(dist_matrix[i][same_cluster & (np.arange(n) != i)])\n",
        "\n",
        "        # b(i): mean nearest-cluster distance\n",
        "        b = float('inf')\n",
        "        for other_label in other_clusters:\n",
        "            other_mask = (labels == other_label)\n",
        "            b = min(b, np.mean(dist_matrix[i][other_mask]))\n",
        "\n",
        "        # Handle edge case if a or b is zero\n",
        "        s = (b - a) / max(a, b) if max(a, b) > 0 else 0\n",
        "        scores.append(s)\n",
        "\n",
        "    return np.mean(scores)"
      ],
      "metadata": {
        "id": "yxxPCIymLVgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    from sklearn.datasets import make_blobs\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=0)\n",
        "\n",
        "    model = KMeans(k=4, random_state=42)\n",
        "    model.fit(X)\n",
        "\n",
        "    score = silhouette_score(X, model.labels)\n",
        "    print(f\"Silhouette Score: {score:.4f}\")"
      ],
      "metadata": {
        "id": "3xj7W9fzLVej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The current implementation of silhouette_score is clear and correct, but it's not optimized — it's O(n²) in time and space due to:\n",
        "# We’ll optimize in two main ways:\n",
        "# ✅ 1. Avoid storing the full distance matrix\n",
        "# We compute only what we need on-the-fly, using:\n",
        "# But this makes each step slower individually, so we need smarter grouping.\n",
        "# ✅ 2. Pre-group points by cluster for efficiency\n",
        "# Create a dictionary mapping each label to its members' indices\n",
        "# Reuse these to compute intra-cluster and inter-cluster distances more efficiently\n",
        "\n",
        "# ⚡ Complexity Comparison:\n",
        "# Version\tTime Complexity\tSpace Complexity\tNotes\n",
        "# Original (full matrix)\tO(n²)\tO(n²)\tFastest for small n\n",
        "# Optimized (on-the-fly)\tO(n²) worst-case\tO(n)\tBetter space usage\n",
        "\n",
        "# ⚡ Further Optimizations (if needed):\n",
        "# Replace inner loops with scipy.spatial.cKDTree or BallTree for faster distance queries.\n",
        "# Parallelize using joblib or numba.\n",
        "# Use mini-batches or sampling for approximate silhouette scores in very large datasets."
      ],
      "metadata": {
        "id": "oi3AImJ1LVc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def silhouette_score_optimized(X, labels):\n",
        "    n = len(X)\n",
        "    unique_labels = np.unique(labels)\n",
        "    k = len(unique_labels)\n",
        "    if k == 1 or k == n:\n",
        "        return 0  # Not meaningful\n",
        "\n",
        "    # Group points by cluster\n",
        "    clusters = {label: np.where(labels == label)[0] for label in unique_labels}\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    for i in range(n):\n",
        "        xi = X[i]\n",
        "        label_i = labels[i]\n",
        "        same_cluster = clusters[label_i]\n",
        "\n",
        "        # Intra-cluster distance (a)\n",
        "        if len(same_cluster) > 1:\n",
        "            a = np.mean([\n",
        "                np.linalg.norm(xi - X[j])\n",
        "                for j in same_cluster if j != i\n",
        "            ])\n",
        "        else:\n",
        "            a = 0  # Only point in its cluster\n",
        "\n",
        "        # Inter-cluster distance (b): find min avg dist to other clusters\n",
        "        b = float('inf')\n",
        "        for other_label, other_indices in clusters.items():\n",
        "            if other_label == label_i:\n",
        "                continue\n",
        "            b_candidate = np.mean([np.linalg.norm(xi - X[j]) for j in other_indices])\n",
        "            if b_candidate < b:\n",
        "                b = b_candidate\n",
        "\n",
        "        # Compute silhouette score for point i\n",
        "        s = (b - a) / max(a, b) if max(a, b) > 0 else 0\n",
        "        scores.append(s)\n",
        "\n",
        "    return np.mean(scores)"
      ],
      "metadata": {
        "id": "ph--PCA8LVZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, _ = make_blobs(n_samples=500, centers=4, cluster_std=0.6, random_state=0)\n",
        "\n",
        "model = KMeans(k=4, random_state=42)\n",
        "model.fit(X)\n",
        "\n",
        "score = silhouette_score_optimized(X, model.labels)\n",
        "print(f\"Optimized Silhouette Score: {score:.4f}\")"
      ],
      "metadata": {
        "id": "p42N7bgKYU1a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}