{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMo4bQGT6HJQI9IcO5VUcWl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inderpreetsingh01/ml_machine_coding/blob/main/DL_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbKjHIRPawAz"
      },
      "outputs": [],
      "source": [
        "# dropout"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Dropout:\n",
        "    def __init__(self, p=0.5):\n",
        "        \"\"\"\n",
        "        Dropout layer.\n",
        "        p : float\n",
        "            Probability of dropping a unit (0 <= p < 1).\n",
        "        \"\"\"\n",
        "        assert 0 <= p < 1, \"Dropout probability must be in [0, 1).\"\n",
        "        self.p = p\n",
        "        self.mask = None\n",
        "        self.training = True  # Default: training mode\n",
        "\n",
        "    def forward(self, X):\n",
        "        if self.training:\n",
        "            # Create mask: 1 with probability (1-p), 0 with probability p\n",
        "            self.mask = (np.random.rand(*X.shape) > self.p).astype(np.float32)\n",
        "\n",
        "            # Scale activations to maintain expected value\n",
        "            return (X * self.mask) / (1.0 - self.p)\n",
        "        else:\n",
        "            # In evaluation mode, just return input\n",
        "            return X\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        \"\"\"\n",
        "        Backprop through dropout (apply same mask).\n",
        "        \"\"\"\n",
        "        if self.training:\n",
        "            return (d_out * self.mask) / (1.0 - self.p)\n",
        "        else:\n",
        "            return d_out\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"Switch to inference mode (no dropout applied).\"\"\"\n",
        "        self.training = False\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Switch to training mode (dropout applied).\"\"\"\n",
        "        self.training = True"
      ],
      "metadata": {
        "id": "KYDagTSBa2FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "dropout = Dropout(p=0.3)\n",
        "\n",
        "X = np.array([[1.0, 2.0, 3.0],\n",
        "              [4.0, 5.0, 6.0]])\n",
        "\n",
        "# Training mode\n",
        "dropout.train()\n",
        "out_train = dropout.forward(X)\n",
        "print(\"Training output:\\n\", out_train)\n",
        "\n",
        "# Evaluation mode\n",
        "dropout.eval()\n",
        "out_eval = dropout.forward(X)\n",
        "print(\"Eval output:\\n\", out_eval)"
      ],
      "metadata": {
        "id": "9JF7S2ODbDEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bu4IU-ojbFt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relu"
      ],
      "metadata": {
        "id": "kZ6Ri8-wbNoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class ReLU:\n",
        "    def __init__(self):\n",
        "        self.mask = None  # stores positions of positive values\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward pass of ReLU activation\"\"\"\n",
        "        self.mask = (X > 0).astype(np.float32)\n",
        "        return X * self.mask\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        \"\"\"Backward pass: pass gradient only where input was > 0\"\"\"\n",
        "        return d_out * self.mask\n"
      ],
      "metadata": {
        "id": "u2DMbhXYbNlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relu = ReLU()\n",
        "\n",
        "X = np.array([[-1, 2, -3],\n",
        "              [4, -5, 6]])\n",
        "\n",
        "# Forward pass\n",
        "out_forward = relu.forward(X)\n",
        "print(\"Forward output:\\n\", out_forward)\n",
        "\n",
        "# Backward pass (gradient from next layer = ones)\n",
        "d_out = np.ones_like(X)\n",
        "out_backward = relu.backward(d_out)\n",
        "print(\"Backward output (gradients):\\n\", out_backward)"
      ],
      "metadata": {
        "id": "kalk-XRsbQ5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fXaJSoGBbTC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TgMWwM1ubZIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid"
      ],
      "metadata": {
        "id": "AKBYWlGWbZGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None  # store forward output for backward\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward pass of Sigmoid activation\"\"\"\n",
        "        self.out = 1 / (1 + np.exp(-X))\n",
        "        return self.out\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        \"\"\"Backward pass: apply derivative of sigmoid\"\"\"\n",
        "        return d_out * (self.out * (1 - self.out))\n"
      ],
      "metadata": {
        "id": "Am5UUTpabZEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid = Sigmoid()\n",
        "\n",
        "X = np.array([[-1, 0, 1],\n",
        "              [2, -2, 3]])\n",
        "\n",
        "# Forward pass\n",
        "out_forward = sigmoid.forward(X)\n",
        "print(\"Forward output:\\n\", out_forward)\n",
        "\n",
        "# Backward pass (gradients from next layer = ones)\n",
        "d_out = np.ones_like(X)\n",
        "out_backward = sigmoid.backward(d_out)\n",
        "print(\"Backward output (gradients):\\n\", out_backward)\n"
      ],
      "metadata": {
        "id": "qQXSfa09bZCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-fXWJcGbt0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dejd7IrwcjoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# softmax"
      ],
      "metadata": {
        "id": "lyMDrBuEcjmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        self.out = None  # store probabilities\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward pass of Softmax activation\"\"\"\n",
        "        # Shift values for numerical stability\n",
        "        shift_X = X - np.max(X, axis=1, keepdims=True)\n",
        "        exp_X = np.exp(shift_X)\n",
        "        self.out = exp_X / np.sum(exp_X, axis=1, keepdims=True)\n",
        "        return self.out\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        \"\"\"\n",
        "        Backward pass of softmax.\n",
        "        d_out: gradient from next layer (usually loss)\n",
        "        \"\"\"\n",
        "        batch_size, num_classes = self.out.shape\n",
        "        d_input = np.empty_like(d_out)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            y = self.out[i].reshape(-1, 1)   # column vector\n",
        "            jacobian = np.diagflat(y) - np.dot(y, y.T)\n",
        "            d_input[i] = np.dot(jacobian, d_out[i])\n",
        "\n",
        "        return d_input"
      ],
      "metadata": {
        "id": "FoKha_6bcjkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = Softmax()\n",
        "\n",
        "X = np.array([[1, 2, 3],\n",
        "              [0.5, 1.5, -1]])\n",
        "\n",
        "# Forward pass\n",
        "out_forward = softmax.forward(X)\n",
        "print(\"Forward output (probabilities):\\n\", out_forward)\n",
        "\n",
        "# Backward pass (gradient from next layer = ones)\n",
        "d_out = np.ones_like(X)\n",
        "out_backward = softmax.backward(d_out)\n",
        "print(\"Backward output (gradients):\\n\", out_backward)"
      ],
      "metadata": {
        "id": "uPkL9lqfcr6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r0wcmgWEcuSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uKscwtcXcvNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# binary cross entropy loss"
      ],
      "metadata": {
        "id": "vSnDLb1McvLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class BinaryCrossEntropyLoss:\n",
        "    def __init__(self):\n",
        "        self.y_true = None\n",
        "        self.y_pred = None\n",
        "\n",
        "    def forward(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Compute binary cross-entropy loss.\n",
        "        y_true: (n_samples,) ground truth labels (0 or 1)\n",
        "        y_pred: (n_samples,) predicted probabilities (0 < p < 1)\n",
        "        \"\"\"\n",
        "        self.y_true = y_true\n",
        "        self.y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)  # prevent log(0)\n",
        "        loss = -np.mean(\n",
        "            y_true * np.log(self.y_pred) + (1 - y_true) * np.log(1 - self.y_pred)\n",
        "        )\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        \"\"\"\n",
        "        Compute gradient of BCE loss w.r.t. predictions.\n",
        "        Gradient is averaged over batch.\n",
        "        \"\"\"\n",
        "        n = self.y_true.shape[0]\n",
        "        grad = (self.y_pred - self.y_true) / (self.y_pred * (1 - self.y_pred) * n)\n",
        "        return grad"
      ],
      "metadata": {
        "id": "nfZXMmeScvJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bce = BinaryCrossEntropyLoss()\n",
        "\n",
        "y_true = np.array([0, 1, 1, 0])  # true labels\n",
        "y_pred = np.array([0.1, 0.9, 0.8, 0.2])  # predicted probs\n",
        "\n",
        "# Forward pass (loss)\n",
        "loss = bce.forward(y_true, y_pred)\n",
        "print(\"BCE Loss:\", loss)\n",
        "\n",
        "# Backward pass (gradients)\n",
        "grad = bce.backward()\n",
        "print(\"Gradients:\", grad)"
      ],
      "metadata": {
        "id": "TciYJT7vcvHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YowTW97TdAqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cross entropy loss"
      ],
      "metadata": {
        "id": "LC9P8vDFdIIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class CrossEntropyLoss:\n",
        "    def __init__(self):\n",
        "        self.y_true = None\n",
        "        self.y_pred = None\n",
        "\n",
        "    def forward(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Compute multi-class cross entropy loss.\n",
        "\n",
        "        y_true: (n_samples,) class indices or (n_samples, n_classes) one-hot\n",
        "        y_pred: (n_samples, n_classes) predicted probabilities (after softmax)\n",
        "        \"\"\"\n",
        "        self.y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        n_samples = y_pred.shape[0]\n",
        "\n",
        "        # If labels are given as class indices\n",
        "        if y_true.ndim == 1:\n",
        "            self.y_true = y_true\n",
        "            log_likelihood = -np.log(self.y_pred[np.arange(n_samples), y_true])\n",
        "            loss = np.mean(log_likelihood)\n",
        "        else:\n",
        "            # One-hot encoded labels\n",
        "            self.y_true = np.argmax(y_true, axis=1)\n",
        "            loss = -np.mean(np.sum(y_true * np.log(self.y_pred), axis=1))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        \"\"\"\n",
        "        Compute gradient of loss w.r.t predictions.\n",
        "        \"\"\"\n",
        "        n_samples = self.y_pred.shape[0]\n",
        "        grad = self.y_pred.copy()\n",
        "        grad[np.arange(n_samples), self.y_true] -= 1\n",
        "        grad /= n_samples\n",
        "        return grad"
      ],
      "metadata": {
        "id": "Kx4FwKAUdIF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax_out = np.array([\n",
        "    [0.1, 0.7, 0.2],\n",
        "    [0.8, 0.1, 0.1],\n",
        "    [0.2, 0.2, 0.6]\n",
        "])\n",
        "\n",
        "# Labels as class indices\n",
        "y_true_idx = np.array([1, 0, 2])\n",
        "\n",
        "# Labels as one-hot\n",
        "y_true_onehot = np.array([\n",
        "    [0, 1, 0],\n",
        "    [1, 0, 0],\n",
        "    [0, 0, 1]\n",
        "])\n",
        "\n",
        "cel = CrossEntropyLoss()\n",
        "\n",
        "# Forward with indices\n",
        "loss_idx = cel.forward(y_true_idx, softmax_out)\n",
        "print(\"Cross-Entropy Loss (indices):\", loss_idx)\n",
        "\n",
        "# Backward pass\n",
        "grad = cel.backward()\n",
        "print(\"Gradients (w.r.t predictions):\\n\", grad)"
      ],
      "metadata": {
        "id": "-Q-hmyPTdLBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SfFM9qR7dK_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# softmax + cross entropy loss\n",
        "import numpy as np\n",
        "\n",
        "class SoftmaxCrossEntropyLoss:\n",
        "    def __init__(self):\n",
        "        self.probs = None\n",
        "        self.y_true = None\n",
        "\n",
        "    def forward(self, logits, y_true):\n",
        "        \"\"\"\n",
        "        Compute softmax + cross entropy loss in one step.\n",
        "\n",
        "        logits: (n_samples, n_classes) raw scores\n",
        "        y_true: (n_samples,) class indices or (n_samples, n_classes) one-hot\n",
        "        \"\"\"\n",
        "        # Shift logits for numerical stability\n",
        "        shift_logits = logits - np.max(logits, axis=1, keepdims=True)\n",
        "        exp_logits = np.exp(shift_logits)\n",
        "        self.probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "\n",
        "        n_samples = logits.shape[0]\n",
        "\n",
        "        if y_true.ndim == 1:  # class indices\n",
        "            self.y_true = y_true\n",
        "            log_likelihood = -np.log(self.probs[np.arange(n_samples), y_true])\n",
        "            loss = np.mean(log_likelihood)\n",
        "        else:  # one-hot encoded\n",
        "            self.y_true = np.argmax(y_true, axis=1)\n",
        "            loss = -np.mean(np.sum(y_true * np.log(self.probs), axis=1))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        \"\"\"\n",
        "        Gradient of softmax + cross entropy w.r.t logits.\n",
        "        \"\"\"\n",
        "        n_samples = self.probs.shape[0]\n",
        "        grad = self.probs.copy()\n",
        "        grad[np.arange(n_samples), self.y_true] -= 1\n",
        "        grad /= n_samples\n",
        "        return grad"
      ],
      "metadata": {
        "id": "ypKIF8M-dK9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = np.array([\n",
        "    [1.0, 2.0, 3.0],\n",
        "    [1.0, 3.0, 0.5],\n",
        "    [2.0, 1.0, 0.1]\n",
        "])\n",
        "\n",
        "# Labels as class indices\n",
        "y_true = np.array([2, 1, 0])\n",
        "\n",
        "sce = SoftmaxCrossEntropyLoss()\n",
        "\n",
        "# Forward pass\n",
        "loss = sce.forward(logits, y_true)\n",
        "print(\"Softmax + CrossEntropy Loss:\", loss)\n",
        "\n",
        "# Backward pass\n",
        "grad = sce.backward()\n",
        "print(\"Gradients (w.r.t logits):\\n\", grad)"
      ],
      "metadata": {
        "id": "vljb1rD6dlg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AdnrZZVVdoME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qqEVUVCzeD9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP"
      ],
      "metadata": {
        "id": "pB27eY6VeD7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# ---------- Utility Functions ----------\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_backward(dout, x):\n",
        "    dx = dout.copy()\n",
        "    dx[x <= 0] = 0\n",
        "    return dx\n",
        "\n",
        "# ---------- Loss: Softmax + CrossEntropy ----------\n",
        "class SoftmaxCrossEntropyLoss:\n",
        "    def __init__(self):\n",
        "        self.probs = None\n",
        "        self.y_true = None\n",
        "\n",
        "    def forward(self, logits, y_true):\n",
        "        # Shift logits for numerical stability\n",
        "        shift_logits = logits - np.max(logits, axis=1, keepdims=True)\n",
        "        exp_logits = np.exp(shift_logits)\n",
        "        self.probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "\n",
        "        n_samples = logits.shape[0]\n",
        "\n",
        "        if y_true.ndim == 1:  # class indices\n",
        "            self.y_true = y_true\n",
        "            log_likelihood = -np.log(self.probs[np.arange(n_samples), y_true])\n",
        "            loss = np.mean(log_likelihood)\n",
        "        else:  # one-hot labels\n",
        "            self.y_true = np.argmax(y_true, axis=1)\n",
        "            loss = -np.mean(np.sum(y_true * np.log(self.probs), axis=1))\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        n_samples = self.probs.shape[0]\n",
        "        grad = self.probs.copy()\n",
        "        grad[np.arange(n_samples), self.y_true] -= 1\n",
        "        grad /= n_samples\n",
        "        return grad\n",
        "\n",
        "# ---------- Simple MLP ----------\n",
        "class MLP:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.1):\n",
        "        self.lr = lr\n",
        "        # Xavier init\n",
        "        self.W1 = np.random.randn(input_dim, hidden_dim) / np.sqrt(input_dim)\n",
        "        self.b1 = np.zeros((1, hidden_dim))\n",
        "        self.W2 = np.random.randn(hidden_dim, output_dim) / np.sqrt(hidden_dim)\n",
        "        self.b2 = np.zeros((1, output_dim))\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        self.z1 = X @ self.W1 + self.b1\n",
        "        self.a1 = relu(self.z1)\n",
        "        self.z2 = self.a1 @ self.W2 + self.b2\n",
        "        return self.z2  # logits\n",
        "\n",
        "    def backward(self, dlogits):\n",
        "        # Gradients for W2, b2\n",
        "        dW2 = self.a1.T @ dlogits\n",
        "        db2 = np.sum(dlogits, axis=0, keepdims=True)\n",
        "\n",
        "        # Backprop through ReLU\n",
        "        da1 = dlogits @ self.W2.T\n",
        "        dz1 = relu_backward(da1, self.z1)\n",
        "\n",
        "        # Gradients for W1, b1\n",
        "        dW1 = self.X.T @ dz1\n",
        "        db1 = np.sum(dz1, axis=0, keepdims=True)\n",
        "\n",
        "        # Update parameters\n",
        "        self.W1 -= self.lr * dW1\n",
        "        self.b1 -= self.lr * db1\n",
        "        self.W2 -= self.lr * dW2\n",
        "        self.b2 -= self.lr * db2\n",
        "\n",
        "# ---------- Training Example ----------\n",
        "np.random.seed(42)\n",
        "\n",
        "# Dummy dataset (4 samples, 2 classes, 2D input)\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([0,1,1,0])  # XOR labels\n",
        "\n",
        "# Model\n",
        "mlp = MLP(input_dim=2, hidden_dim=4, output_dim=2, lr=0.1)\n",
        "criterion = SoftmaxCrossEntropyLoss()\n",
        "\n",
        "# Training\n",
        "for epoch in range(200):\n",
        "    logits = mlp.forward(X)\n",
        "    loss = criterion.forward(logits, y)\n",
        "    grad = criterion.backward()\n",
        "    mlp.backward(grad)\n",
        "\n",
        "    if (epoch+1) % 50 == 0:\n",
        "        preds = np.argmax(criterion.probs, axis=1)\n",
        "        acc = np.mean(preds == y)\n",
        "        print(f\"Epoch {epoch+1}, Loss={loss:.4f}, Acc={acc:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzmdHbpqeD5I",
        "outputId": "f6edf871-62f3-4b49-bd9e-27085e25d0d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50, Loss=0.5897, Acc=0.75\n",
            "Epoch 100, Loss=0.4609, Acc=1.00\n",
            "Epoch 150, Loss=0.3563, Acc=1.00\n",
            "Epoch 200, Loss=0.2676, Acc=1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "947lxvo5ixI4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}